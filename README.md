ExGAP: Enhancing Explainability with Large Language Models Agnostic Prompting

Artificial Intelligence (AI) is increasingly embedded in everyday life, yet often lacks transparency in interpreting its results, adversely affecting users' comprehension and trust in AI-driven decisions. Although the field of eXplainable Artificial Intelligence (XAI) seeks to address these concerns, current methods frequently deliver explanations that are neither human-friendly nor meaningful. In this work, we introduce ExGAP, a data-, model-, and explanation-agnostic framework, turning Large Language Models (LLMs) into explainers to improve transparency and interpretability in AI applications. ExGAP employs in-context learning to convey contextual information from prevalent XAI methods into LLMs, yielding explanations that are comprehensible and aligned with human understanding. We evaluate the structure and content quality of ExGAP explanations, finding a strong correlation with ground-truth explanations (Spearman rank correlation=0.92 for feature importance rankings). Additionally, a mixed-methods user study (N=56) demonstrates significantly improved pragmatic and hedonic quality for ExGAP, highlighting its effectiveness in generating human-friendly explanations.
